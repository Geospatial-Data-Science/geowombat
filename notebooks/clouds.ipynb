{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import geowombat as gw\n",
    "from geowombat.models import CloudClassifier\n",
    "\n",
    "import satsmooth\n",
    "\n",
    "from mpglue.classification._moving_window import moving_window\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import rasterio as rio\n",
    "import numba as nb\n",
    "from skimage.exposure import rescale_intensity\n",
    "import sklearn_crfsuite\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from affine import Affine\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import pymorph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit\n",
    "def sample_to_dict(tsp, *args):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tsp (1d)\n",
    "    \"\"\"\n",
    "\n",
    "    nt = len(tsp)\n",
    "    \n",
    "    if args:\n",
    "\n",
    "        n_args = len(args)\n",
    "\n",
    "        tlab = np.empty(nt+n_args, dtype=object)\n",
    "        tval = np.zeros(nt+n_args, dtype='float64')\n",
    "\n",
    "    else:\n",
    "\n",
    "        tlab = np.empty(nt, dtype=object)\n",
    "        tval = np.zeros(nt, dtype='float64')    \n",
    "    \n",
    "    for r in range(0, nt):\n",
    "        \n",
    "        tlab[r] = str(r+1).encode('utf-8')    \n",
    "        tval[r] = tsp[r]\n",
    "    \n",
    "    if args:\n",
    "\n",
    "        for i, (arg_name, arg_value) in enumerate(args):\n",
    "\n",
    "            tlab[r+i+1] = arg_name.encode('utf-8')\n",
    "            tval[r+i+1] = arg_value    \n",
    "    \n",
    "    return dict(zip(tlab.tolist(), tval))\n",
    "\n",
    "\n",
    "def _nan_check(value):\n",
    "    return 0.0 if np.isnan(value) else value\n",
    "\n",
    "\n",
    "def _min(a, b):\n",
    "    return a if a < b else b\n",
    "\n",
    "\n",
    "def _max(a, b):\n",
    "    return a if a > b else b\n",
    "\n",
    "\n",
    "def _clip_low(value):\n",
    "    return 0.0 if value < 0 else value\n",
    "\n",
    "\n",
    "def _clip_high(value):\n",
    "    return 1.0 if value > 1 else value\n",
    "\n",
    "\n",
    "def _clip(value):\n",
    "    return _clip_low(value) if value < 1 else _clip_high(value)\n",
    "\n",
    "\n",
    "def _evi(blue, red, nir):\n",
    "    \"\"\"Enhanced Vegetation Index\"\"\"\n",
    "    return 2.5 * ((nir - red) / (nir + 6.0 * red - 7.5 * blue + 1.0))\n",
    "\n",
    "\n",
    "def _evi2(red, nir):\n",
    "    \"\"\"Two-band Enhanced Vegetation Index\"\"\"\n",
    "    return 2.5 * ((nir - red) / (nir + 1.0 + (2.4 * red)))\n",
    "\n",
    "\n",
    "def _bsi(blue, red, nir, swir2):\n",
    "    \"\"\"Bare Soil Index\"\"\"\n",
    "    return ((swir2 + red) - (nir - blue)) / ((swir2 + red) + (nir - blue))\n",
    "\n",
    "\n",
    "def _brightness(green, red, nir):\n",
    "    \"\"\"Brightness Index\"\"\"\n",
    "    return (green**2 + red**2 + nir**2)**0.5\n",
    "\n",
    "\n",
    "def _brightness_swir(green, red, nir, swir1):\n",
    "    \"\"\"Brightness Index\"\"\"\n",
    "    return (green**2 + red**2 + nir**2 + swir1**2)**0.5\n",
    "\n",
    "\n",
    "def _dbsi(green, red, nir, swir1):\n",
    "    \"\"\"Dry Bare Soil Index\"\"\"\n",
    "    return ((swir1 - green) / (swir1 + green)) - _ndvi(red, nir)\n",
    "\n",
    "\n",
    "def _nbr(nir, swir2):\n",
    "    \"\"\"Normalized Burn Ratio\"\"\"\n",
    "    return (nir - swir2) / (nir + swir2)\n",
    "\n",
    "\n",
    "def _ndmi(nir, swir1):\n",
    "    \"\"\"Normalized Difference Moisture Index\"\"\"\n",
    "    return (nir - swir1) / (nir + swir1)\n",
    "\n",
    "\n",
    "def _ndvi(red, nir):\n",
    "    \"\"\"Normalized Difference Vegetation Index\"\"\"\n",
    "    return (nir - red) / (nir + red)\n",
    "\n",
    "\n",
    "def _wi(red, swir1):\n",
    "    \"\"\"Woody Index\"\"\"\n",
    "    return 0.0 if red + swir1 > 0.5 else 1.0 - ((red + swir1) / 0.5)\n",
    "\n",
    "    \n",
    "SENSOR_BANDS = dict(l7=dict(blue=0,\n",
    "                            green=1,\n",
    "                            red=2,\n",
    "                            nir=3,\n",
    "                            swir1=4,\n",
    "                            swir2=5),\n",
    "                    l8=dict(blue=0,\n",
    "                            green=1,\n",
    "                            red=2,\n",
    "                            nir=3,\n",
    "                            swir1=4,\n",
    "                            swir2=5),\n",
    "                    l5bgrn=dict(blue=0,\n",
    "                                green=1,\n",
    "                                red=2,\n",
    "                                nir=3),\n",
    "                    l7bgrn=dict(blue=0,\n",
    "                                green=1,\n",
    "                                red=2,\n",
    "                                nir=3),\n",
    "                    l8bgrn=dict(blue=0,\n",
    "                                green=1,\n",
    "                                red=2,\n",
    "                                nir=3),\n",
    "                    bgrn=dict(blue=0,\n",
    "                              green=1,\n",
    "                              red=2,\n",
    "                              nir=3),\n",
    "                    qb=dict(blue=0,\n",
    "                            green=1,\n",
    "                            red=2,\n",
    "                            nir=3),\n",
    "                    ps=dict(blue=0,\n",
    "                            green=1,\n",
    "                            red=2,\n",
    "                            nir=3),\n",
    "                    s210=dict(blue=0,\n",
    "                              green=1,\n",
    "                              red=2,\n",
    "                              nir=3),\n",
    "                    s220=dict(nir1=0,\n",
    "                              nir2=1,\n",
    "                              nir3=2,\n",
    "                              rededge=3,\n",
    "                              swir1=4,\n",
    "                              swir2=5),\n",
    "                    s2l7=dict(blue=0,\n",
    "                              green=1,\n",
    "                              red=2,\n",
    "                              nir=3,\n",
    "                              swir1=4,\n",
    "                              swir2=5))    \n",
    "\n",
    "# @nb.jit\n",
    "def array_to_dict(sensor, *args):\n",
    "    \n",
    "    \"\"\"\n",
    "    Converts an array sample to a CRF features\n",
    "    \"\"\"\n",
    "    \n",
    "    nargs = len(args)\n",
    "    feas = list()\n",
    "    \n",
    "    if sensor != 'pan':\n",
    "        \n",
    "        if sensor in ['s210', 'l5bgrn', 'l7bgrn', 'l8bgrn', 'bgrn', 'qb', 'ps']:\n",
    "\n",
    "            blue_idx = SENSOR_BANDS[sensor]['blue']\n",
    "            green_idx = SENSOR_BANDS[sensor]['green']\n",
    "            red_idx = SENSOR_BANDS[sensor]['red']\n",
    "            nir_idx = SENSOR_BANDS[sensor]['nir']\n",
    "            \n",
    "        elif sensor == 's220':\n",
    "\n",
    "            nir1_idx = SENSOR_BANDS[sensor]['nir1']\n",
    "            nir2_idx = SENSOR_BANDS[sensor]['nir2']\n",
    "            nir3_idx = SENSOR_BANDS[sensor]['nir3']\n",
    "            rededge_idx = SENSOR_BANDS[sensor]['rededge']\n",
    "            swir1_idx = SENSOR_BANDS[sensor]['swir1']\n",
    "            swir2_idx = SENSOR_BANDS[sensor]['swir2']\n",
    "            \n",
    "        else:\n",
    "\n",
    "            blue_idx = SENSOR_BANDS[sensor]['blue']\n",
    "            green_idx = SENSOR_BANDS[sensor]['green']\n",
    "            red_idx = SENSOR_BANDS[sensor]['red']\n",
    "            nir_idx = SENSOR_BANDS[sensor]['nir']\n",
    "            swir1_idx = SENSOR_BANDS[sensor]['swir1']\n",
    "            swir2_idx = SENSOR_BANDS[sensor]['swir2']\n",
    "\n",
    "    for si in range(0, nargs):\n",
    "        \n",
    "        tsamp = args[si]*0.0001\n",
    "\n",
    "        if sensor == 'pan':            \n",
    "            feas.append(sample_to_dict(tsamp))            \n",
    "        else:\n",
    "            \n",
    "            if sensor in ['s210', 'l5bgrn', 'l7bgrn', 'l8bgrn', 'bgrn', 'qb', 'ps']:\n",
    "            \n",
    "                brightness = _brightness(tsamp[green_idx], tsamp[red_idx], tsamp[nir_idx])\n",
    "                evi = _evi(tsamp[blue_idx], tsamp[red_idx], tsamp[nir_idx])                \n",
    "                evi2 = _evi2(tsamp[red_idx], tsamp[nir_idx])\n",
    "                gndvi = _ndvi(tsamp[green_idx], tsamp[nir_idx])\n",
    "                ndvi = _ndvi(tsamp[red_idx], tsamp[nir_idx])\n",
    "                \n",
    "                indices = [('brightness', _nan_check(brightness)),\n",
    "                           ('evi', _nan_check(evi)),\n",
    "                           ('evi2', _nan_check(evi2)),\n",
    "                           ('gndvi', _nan_check(gndvi)),\n",
    "                           ('ndvi', _nan_check(ndvi))]\n",
    "                \n",
    "            elif sensor == 's220':\n",
    "                \n",
    "                brightness = _brightness(tsamp[nir1_idx], tsamp[rededge_idx], tsamp[swir1_idx])                \n",
    "                nbr = _nbr(tsamp[rededge_idx], tsamp[swir2_idx])\n",
    "                ndvi = _ndvi(tsamp[nir1_idx], tsamp[rededge_idx])                        \n",
    "                wi = _wi(tsamp[nir1_idx], tsamp[swir1_idx])            \n",
    "                \n",
    "                indices = [('bri', _nan_check(brightness)),\n",
    "                           ('nbr', _nan_check(nbr)),\n",
    "                           ('ndvi', _nan_check(ndvi)),\n",
    "                           ('wi', _nan_check(wi))]\n",
    "                \n",
    "            else:\n",
    "\n",
    "                brightness = _brightness_swir(tsamp[green_idx], tsamp[red_idx], tsamp[nir_idx], tsamp[swir1_idx])\n",
    "                #dbsi = _dbsi(tsamp[green_idx], tsamp[red_idx], tsamp[nir_idx], tsamp[swir1_idx])\n",
    "                evi = _evi(tsamp[blue_idx], tsamp[red_idx], tsamp[nir_idx])\n",
    "                evi2 = _evi2(tsamp[red_idx], tsamp[nir_idx])\n",
    "                gndvi = _ndvi(tsamp[green_idx], tsamp[nir_idx])\n",
    "                nbr = _nbr(tsamp[nir_idx], tsamp[swir2_idx])\n",
    "                ndmi = _ndmi(tsamp[nir_idx], tsamp[swir1_idx])\n",
    "                ndvi = _ndvi(tsamp[red_idx], tsamp[nir_idx])                        \n",
    "                wi = _wi(tsamp[red_idx], tsamp[swir1_idx])            \n",
    "                \n",
    "                indices = [('bri', _nan_check(brightness)),\n",
    "                           ('evi', _clip(_nan_check(evi))),\n",
    "                           ('evi2', _clip(_nan_check(evi2))),\n",
    "                           ('gndvi', _nan_check(gndvi)),\n",
    "                           ('nbr', _nan_check(nbr)),\n",
    "                           ('ndmi', _nan_check(ndmi)),\n",
    "                           ('ndvi', _nan_check(ndvi)),\n",
    "                           ('wi', _clip(_nan_check(wi)))]\n",
    "\n",
    "            feas.append(sample_to_dict(tsamp, *indices))\n",
    "\n",
    "    return feas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id label                                              image  \\\n",
      "0  None     c  na/ca/brdf/L1C_T10SFH_A007146_20180719T185932_...   \n",
      "1  None     c  na/ca/brdf/L1C_T10SFH_A007146_20180719T185932_...   \n",
      "2  None     c  na/ca/brdf/L1C_T10SFH_A007146_20180719T185932_...   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((-2055860.752 52462.792, -2055860.752...  \n",
      "1  POLYGON ((-2058787.329 51671.032, -2058787.329...  \n",
      "2  POLYGON ((-2056253.821 55985.850, -2056253.821...  \n",
      "(66, 4)\n"
     ]
    }
   ],
   "source": [
    "region = 'na'\n",
    "\n",
    "ref_res = (20, 20)\n",
    "\n",
    "# North America\n",
    "naaeac_proj = \"+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\"\n",
    "\n",
    "# South America\n",
    "saaeac_proj = \"+proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0 +y_0=0 +ellps=aust_SA +units=m +no_defs\"\n",
    "\n",
    "# Queensland\n",
    "aus_proj = \"+proj=aea +lat_1=-12 +lat_2=-28 +lat_0=-28 +lon_0=145 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\"\n",
    "\n",
    "proj_dict = dict(na=naaeac_proj,\n",
    "                 sa=saaeac_proj,\n",
    "                 aus=aus_proj)\n",
    "\n",
    "ref_crs = proj_dict[region]\n",
    "\n",
    "# Sentinel 2\n",
    "# rpath = Path('/scratch/rsc4/graesser/temp/s2/grids')\n",
    "# vpath = Path('/scratch/rsc4/graesser/temp/s2/training/s2_training.shp')\n",
    "\n",
    "# RGBN cubes\n",
    "rpath = Path('/media/jcgr/data/projects/global_fields/data/grids')\n",
    "vpath = Path('/media/jcgr/data/projects/global_fields/data/training/clouds/{}.shp'.format(region))\n",
    "\n",
    "df = gpd.read_file(vpath.as_posix())\n",
    "\n",
    "print(df.head(3))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_ext = '.img'\n",
    "# replace_a = ''  # 'sa/'\n",
    "# replace_b = ''  # 'sa_sp/'\n",
    "\n",
    "image_ext = ''\n",
    "replace_a = '{}/'.format(region)\n",
    "replace_b = '{}_sp/'.format(region)\n",
    "\n",
    "minrow = 1e9\n",
    "mincol = 1e9\n",
    "\n",
    "X_data = list()\n",
    "y_data = list()\n",
    "\n",
    "#df = df.replace(to_replace='u', value='l')\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# le.fit(df.label.values)\n",
    "# df['label_value'] = le.transform(df.label.values)\n",
    "\n",
    "with gw.config.update(ref_crs=ref_crs):\n",
    "\n",
    "    for row in tqdm(df.itertuples(index=True, name='Pandas'), total=df.shape[0]):\n",
    "\n",
    "        rimage = rpath.joinpath(row.image.replace(replace_a, replace_b) + image_ext)\n",
    "        \n",
    "        if rimage.is_file():\n",
    "\n",
    "            with gw.open(rimage.as_posix(), chunks=512) as ds:\n",
    "\n",
    "                clip = gw.clip(ds, \n",
    "                               df, \n",
    "                               query=\"index == {:d}\".format(row.Index), \n",
    "                               mask_data=False)\n",
    "\n",
    "                subset = clip.clip(0, 10000).data.compute()\n",
    "                minrow = min(subset.shape[1], minrow)\n",
    "                mincol = min(subset.shape[2], mincol)\n",
    "                X_data.append(subset)\n",
    "                y_data.append(row.label)\n",
    "    \n",
    "X_data = np.array([d[:, :minrow, :mincol] for d in X_data], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5\n",
    "sensor = 'l7'\n",
    "\n",
    "max_rand_length = 10\n",
    "\n",
    "ntime, nbands, nrows, ncols = X_data.shape\n",
    "features = list()\n",
    "\n",
    "X = list()\n",
    "y = list()\n",
    "\n",
    "for iter_ in trange(0, n_iter):\n",
    "\n",
    "    for a in [['l', 'u'], ['u', 'w'], ['l', 'w']]:\n",
    "\n",
    "        Xd = list()\n",
    "        yd = list()\n",
    "\n",
    "        idx_null = np.array([ij for ij, cl in enumerate(y_data) if cl not in a], dtype='int64')\n",
    "\n",
    "        ntime_null = idx_null.shape[0]\n",
    "\n",
    "        low = np.random.randint(0, high=int(ntime_null/2))\n",
    "        high = np.random.randint(low+2, high=ntime_null)\n",
    "        idx_random = np.array([i + 1 for i in range(low, high)], dtype='int64')\n",
    "\n",
    "        n_rand = min(len(idx_random), max_rand_length)\n",
    "\n",
    "        for i in range(0, n_rand):\n",
    "\n",
    "            # Get a random subset of temporal indices\n",
    "            idx = idx_null[np.random.choice(idx_random, size=n_rand, replace=False)]\n",
    "\n",
    "            # Transpose each temporal state --> samples x features\n",
    "            Xd_ = [dlayer.transpose(1, 2, 0).reshape(nrows*ncols, nbands) for dlayer in X_data[idx]]\n",
    "\n",
    "            # Flatten the data from [time x features x rows x columns] --> [s1, s2, ..., sn]\n",
    "            # len(Xd_) = n samples\n",
    "            # len(Xd_[0]) = n time\n",
    "            Xd_ = [array_to_dict(sensor, *[Xd_[j][i] for j in range(0, idx.shape[0])]) for i in range(0, nrows*ncols)]\n",
    "\n",
    "            # len(y_) = n samples\n",
    "            # len(y_[0]) = n time        \n",
    "            y_ = [np.array(y_data)[idx].tolist() for i in range(0, nrows*ncols)]\n",
    "\n",
    "            Xd += Xd_\n",
    "            yd += y_\n",
    "\n",
    "        # time series of len(idx) x n_rand x n_rand\n",
    "        #Xd = np.vstack(Xd)\n",
    "        #niter = Xd.shape[0]\n",
    "\n",
    "    #     Xd = [dlayer.transpose(1, 2, 0).reshape(nrows*ncols, nbands) for dlayer in Xd]\n",
    "    #     Xd = [array_to_dict(*[Xd[j][i] for j in range(0, niter)]) for i in range(0, nrows*ncols)]\n",
    "\n",
    "    #     yd = list(itertools.chain(*yd))\n",
    "    #     yd = [[y_ for y_ in yd] for i in range(0, nrows*ncols)]\n",
    "\n",
    "        X += Xd\n",
    "        y += yd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of random arrangements:', n_iter)\n",
    "print('Number of samples:', nrows*ncols)\n",
    "print('Number of random arrangements x n samples:', len(X))\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent using the limited-memory BFGS method (with L1 and L2 regularization)\n",
    "model = CloudClassifier(algorithm='lbfgs', \n",
    "                        c1=0.01,\n",
    "                        c2=0.01,\n",
    "                        max_iterations=1000,\n",
    "                        num_memories=20,\n",
    "                        epsilon=0.001,\n",
    "                        delta=0.001,\n",
    "                        period=20,\n",
    "                        linesearch='MoreThuente',  # 'MoreThuente' 'Backtracking' 'StrongBacktracking'\n",
    "                        max_linesearch=20,\n",
    "                        all_possible_states=True,\n",
    "                        all_possible_transitions=True,\n",
    "                        verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_file('/media/jcgr/data/projects/global_fields/data/grids/models/clouds_brgnss.model', overwrite=True)\n",
    "# model.to_file('/scratch/rsc4/graesser/temp/clouds.model', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel 2\n",
    "# rpath = Path('/scratch/rsc4/graesser/temp/s2/grids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/media/jcgr/data/projects/global_fields/data/grids/na_sp')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpath_pred = rpath.joinpath('na_sp')\n",
    "# rpath_pred = rpath.joinpath('t56jmq')\n",
    "rpath_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CloudClassifier()\n",
    "model.from_file('/media/jcgr/data/projects/global_fields/data/grids/models/clouds_bgrn.model')\n",
    "# model.from_file('/scratch/rsc4/graesser/temp/clouds_v2.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the image dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_pos = 2\n",
    "# wildcard = '*_abbm*.img'\n",
    "\n",
    "location = 'ca'\n",
    "\n",
    "date_pos = 3\n",
    "wildcard = '*.tif'\n",
    "\n",
    "filenames = list()\n",
    "time_names = list()\n",
    "\n",
    "for root, dirs, files in os.walk(rpath_pred.as_posix()):\n",
    "    tif_files = fnmatch.filter(files, wildcard)\n",
    "    if tif_files and ('/{}/'.format(location) in root):\n",
    "        \n",
    "        filename_dict = dict()\n",
    "        for fn in tif_files:\n",
    "            filename_dict[fn.split('_')[date_pos]] = fn\n",
    "        \n",
    "        sorted_names = list(zip(*sorted(filename_dict.items())))\n",
    "        \n",
    "        filenames += [Path(root).joinpath(fn).as_posix() for fn in list(sorted_names[1])]\n",
    "        time_names += [datetime.strptime(t[:8], '%Y%m%d') for t in list(sorted_names[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A015840_20180704T185545_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A006960_20180706T185021_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007003_20180709T185935_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A015940_20180711T184212_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A015940_20180711T185352_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/LC08_L1TP_043034_20180712_20180717_01_T1.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A015983_20180714T190152_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007103_20180716T185042_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007146_20180719T185932_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A016083_20180721T185128_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A016126_20180724T185613_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007246_20180726T185132_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/LC08_L1TP_043034_20180728_20180813_01_T1.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007289_20180729T185102_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007289_20180729T190336_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A016226_20180731T184407_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A016269_20180803T185257_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007389_20180805T184622_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007432_20180808T190114_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A016369_20180810T185321_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A016412_20180813T185918_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007532_20180815T185154_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007575_20180818T185858_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A016512_20180820T184735_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A016555_20180823T185556_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007675_20180825T184430_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A007718_20180828T185519_MTD.tif',\n",
       " '/media/jcgr/data/projects/global_fields/data/grids/na_sp/ca/brdf/L1C_T10SFH_A016655_20180830T185331_MTD.tif']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:39:40:WARNING:404:xarray_.concat:  The band list length does not match the sensor bands.\n"
     ]
    }
   ],
   "source": [
    "sensor = 'l7' # 'l7pan'\n",
    "s = 128\n",
    "t = 5\n",
    "y = 54206.53\n",
    "x = -2053578.26\n",
    "\n",
    "pred_kwargs = dict(count=1,\n",
    "                   dtype='uint8',\n",
    "                   nodata=0,\n",
    "                   driver='GTiff',\n",
    "                   blockxsize=256,\n",
    "                   blockysize=256,\n",
    "                   tiled=True,\n",
    "                   compress='lzw')\n",
    "\n",
    "cell_size = 20.0\n",
    "\n",
    "with gw.open(filenames[0]) as ds:\n",
    "    crs = ds.crs\n",
    "    transform = ds.transform\n",
    "    \n",
    "with gw.config.update(sensor=sensor,\n",
    "                      ref_crs=ref_crs,\n",
    "                      ref_res=(cell_size, cell_size),\n",
    "                      ref_bounds=(x, y-(cell_size*s), x+(cell_size*s), y)):\n",
    "    \n",
    "    with gw.open(filenames[:t], \n",
    "                 time_names=time_names[:t],\n",
    "                 chunks=512,\n",
    "                 num_threads=8) as ds:\n",
    "        \n",
    "        pred_kwargs['crs'] = crs\n",
    "        pred_kwargs['transform'] = transform            \n",
    "        \n",
    "        sat_bands = (ds.astype('float64') * 0.0001).clip(0, 1).data.compute(num_workers=8)\n",
    "        \n",
    "for band in range(0, sat_bands.shape[1]):\n",
    "\n",
    "    sat_bands[:, band, :, :] = satsmooth.spatial_temporal(np.ascontiguousarray(sat_bands[:, band, :, :], dtype='float32'),\n",
    "                                          k=3,\n",
    "                                          t=3,\n",
    "                                          sigma_time=0.0,\n",
    "                                          sigma_color=0.1,\n",
    "                                          sigma_space=0.1,\n",
    "                                          n_jobs=8)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict clouds and shadows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geowombat.models.model import time_to_crffeas\n",
    "\n",
    "# ntime, nbands, nrows, ncols = sat_bands.shape\n",
    "\n",
    "# features = np.ascontiguousarray([tlayer.transpose(1, 2, 0).reshape(nrows * ncols,\n",
    "#                                                                    nbands)\n",
    "#                                  for tlayer in sat_bands], dtype='float64')\n",
    "\n",
    "# features = time_to_crffeas(features,\n",
    "#                            b'l7',\n",
    "#                            ntime,\n",
    "#                            nrows,\n",
    "#                            ncols,\n",
    "#                            scale_factor=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_probas(sat_bands, 'l7', scale_factor=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntime, nbands, nrows, ncols = pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kwargs['width'] = ncols\n",
    "pred_kwargs['height'] = nrows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability label relaxation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_c = pred.copy()\n",
    "# for pidx, pred_layer in enumerate(pred):\n",
    "# #     pred_layer = moving_window(pred_layer,\n",
    "# #                                statistic='plr',\n",
    "# #                                window_size=5,\n",
    "# #                                weights=None,\n",
    "# #                                iterations=1)    \n",
    "#     for pidxs, pred_layer_sub in enumerate(pred_layer):\n",
    "#         pred_layer[pidxs] = cv2.bilateralFilter(np.float32(pred_layer_sub), 3, 1.0, 1.0)\n",
    "#     pred_c[pidx] = pred_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, dpi=120, figsize=(6, 10))\n",
    "\n",
    "tplot = 4\n",
    "\n",
    "classes = ['c', 'l', 's', 'u', 'w']\n",
    "\n",
    "# Time n, mean over bands\n",
    "# rgb = np.squeeze(test[tplot].copy())\n",
    "# rgb = rescale_intensity(rgb, out_range=(np.percentile(rgb, 5), np.percentile(rgb, 95)))\n",
    "\n",
    "rgb = sat_bands[tplot][:3].copy()\n",
    "for i in range(0, 3):\n",
    "    rgb[i] = rescale_intensity(rgb[i], out_range=(np.percentile(rgb[i], 5), np.percentile(rgb[i], 95)))\n",
    "rgb = rgb.transpose(1, 2, 0)\n",
    "rgb = rgb[..., ::-1]\n",
    "\n",
    "rgb /= rgb.max()\n",
    "\n",
    "axes[0][0].imshow(rgb)\n",
    "axes[0][0].axis('off')\n",
    "\n",
    "# Time n, predicted probabilities for class k\n",
    "axes[0][1].imshow(pred_c[tplot][classes.index('l')], vmin=0, vmax=1, cmap='plasma')\n",
    "axes[0][1].set_title('P(Land)')\n",
    "axes[0][1].axis('off')\n",
    "\n",
    "axes[1][0].imshow(pred_c[tplot][classes.index('c')], vmin=0, vmax=1, cmap='plasma')\n",
    "axes[1][0].set_title('P(Cloud)')\n",
    "axes[1][0].axis('off')\n",
    "\n",
    "axes[1][1].imshow(pred_c[tplot][classes.index('s')], vmin=0, vmax=1, cmap='plasma')\n",
    "axes[1][1].set_title('P(Shadow)')\n",
    "axes[1][1].axis('off')\n",
    "\n",
    "axes[2][0].imshow(pred_c[tplot][classes.index('w')], vmin=0, vmax=1, cmap='plasma')\n",
    "axes[2][0].set_title('P(Water)')\n",
    "axes[2][0].axis('off')\n",
    "\n",
    "# pred_int = np.where(pred[tplot][classes.index('c')] >= pthresh, 1, \n",
    "#                     np.where(pred[tplot][classes.index('s')] >= pthresh, 2, 0))\n",
    "\n",
    "pred_int = pred_c[tplot].argmax(axis=0) + 1\n",
    "\n",
    "# pred_int = np.where((amax == classes.index('c')) & (pred[tplot][classes.index('c')] > pthresh), 1, \n",
    "#                     np.where((amax == classes.index('s')) & (pred[tplot][classes.index('s')] > pthresh), 2, 0))\n",
    "\n",
    "kernel_cross = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]], dtype='uint8')\n",
    "kernel_square = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype='uint8')\n",
    "\n",
    "pred_int = pymorph.closerec(pymorph.closerec(pred_int,\n",
    "                                             Bdil=pymorph.secross(r=3),\n",
    "                                             Bc=pymorph.secross(r=1)),\n",
    "                            Bdil=pymorph.secross(r=2),\n",
    "                            Bc=pymorph.secross(r=1))\n",
    "\n",
    "# pred_int = cv2.erode(np.uint8(pred_int), kernel_square, iterations=1)\n",
    "# pred_int = cv2.dilate(np.uint8(pred_int), kernel_square, iterations=3)\n",
    "\n",
    "axes[2][1].imshow(pred_int, vmin=0, vmax=5, cmap='jet')\n",
    "axes[2][1].set_title('Prediction')\n",
    "axes[2][1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the predictions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_file = '/scratch/rsc4/graesser/temp/clouds_{}_x{:.3f}_y{:.3f}.tif'.format(Path(filenames[tplot]).name.replace('.img', ''), x, y)\n",
    "\n",
    "print(cloud_file)\n",
    "\n",
    "if Path(cloud_file).is_file():\n",
    "    os.remove(cloud_file)\n",
    "\n",
    "with rio.open(cloud_file, mode='w', **pred_kwargs) as dst:\n",
    "    dst.write(np.uint8(pred_int), indexes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download PlanetScope data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_data('/scratch/rsc4/graesser/temp/planet',\n",
    "#               '/export/home/graesserj/code/python/github/planet/tsplanet/geojson/qld.geojson',\n",
    "#               '2019-08-01', \n",
    "#               '2019-08-10',\n",
    "#               cloud_cover=0.8,\n",
    "#               api_key='des')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract samples from training AOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_names(grid, filenames):\n",
    "    \n",
    "    rpath_dict = {'t55jel': '/scratch/rsc8/hardtkel/biocondition/biocondition/layers/sentinel/{}'.format(grid),\n",
    "                  't56jkt': '/scratch/rsc8/hardtkel/biocondition/biocondition/layers/sentinel/{}'.format(grid),\n",
    "                  't55kcu': '/scratch/rsc8/hardtkel/rapidfires/LW/{}'.format(grid),\n",
    "                  'l8': '/media/jcgr/data/imagery/google/p225r083/brdf'}\n",
    "    \n",
    "    NameInfo = namedtuple('NameInfo', 'rasters vector, time_names')\n",
    "    \n",
    "    #vector_dir = Path('/scratch/rsc4/graesser/temp/s2/training')\n",
    "    vector_dir = Path('/media/jcgr/data/imagery/google/training')\n",
    "    \n",
    "    image_dir = Path(rpath_dict[grid])\n",
    "    \n",
    "    filename_dict = dict()\n",
    "    for fn in filenames:\n",
    "        filename_dict[fn.split('_')[2]] = fn\n",
    "\n",
    "    sorted_names = list(zip(*sorted(filename_dict.items())))\n",
    "    \n",
    "    filenames = list(sorted_names[1])\n",
    "    \n",
    "    time_names = [datetime.strptime(t, '%Y%m%d') for t in list(sorted_names[0])]\n",
    "    \n",
    "    rasters = [image_dir.joinpath(fn).as_posix() for fn in filenames]\n",
    "    vector = vector_dir.joinpath('{}.shp'.format(grid)).as_posix()\n",
    "    \n",
    "    return NameInfo(rasters=rasters, vector=vector, time_names=time_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = fnmatch.filter(os.listdir(image_dir.as_posix()), '*.tif')\n",
    "\n",
    "filename_dict = dict()\n",
    "for fn in filenames:\n",
    "    filename_dict[fn.split('_')[3]] = fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_names = list(zip(*sorted(filename_dict.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = list(sorted_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_names = [datetime.strptime(t, '%Y%m%d') for t in list(sorted_names[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasters = [image_dir.joinpath(fn).as_posix() for fn in filenames]\n",
    "vector = vector_dir.joinpath('p225r83.shp').as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with gw.config.update(ref_image=rasters[0], \n",
    "#                       ref_res=(10, 10), \n",
    "#                       sensor='planetscope'):\n",
    "    \n",
    "#     with gw.open(rasters,\n",
    "#                  time_names=time_names,\n",
    "#                  how='intersection',\n",
    "#                  chunks=512) as ds:\n",
    "        \n",
    "#         dss = ds.mean(dim='time')\n",
    "#         dss.attrs = ds.attrs\n",
    "#         print(ds)\n",
    "        \n",
    "#         #dss.gw.imshow(band_names=['red', 'green', 'blue'], nodata=0, robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(vector)\n",
    "\n",
    "df['class_len'] = df['class'].str.len()\n",
    "df['int_class'] = df.apply(str_to_int, axis=1)\n",
    "out = df['int_class'].str.split(',', expand=True)\n",
    "\n",
    "df_list = list()\n",
    "\n",
    "for s in range(0, out.shape[1]):\n",
    "    df_ = df.copy()\n",
    "    df_['id'] = out.iloc[:, s]\n",
    "    df_list.append(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = list()\n",
    "\n",
    "with gw.config.update(sensor='bgrn'):\n",
    "\n",
    "    with gw.open(rasters[0]) as ds:\n",
    "        transform = ds.transform\n",
    "\n",
    "    with gw.open(rasters, time_names=time_names) as ds:    \n",
    "\n",
    "        for ti, time in enumerate(ds.time.values):\n",
    "\n",
    "            dss = ds.sel(time=time)\n",
    "            dss.attrs = ds.attrs\n",
    "            dss.attrs['transform'] = transform\n",
    "\n",
    "            dfs.append(dss.gw.extract(df_list[ti]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert samples to CRF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_names = ['blue', 'green', 'red', 'nir']\n",
    "label_name = 'poly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = samples_to_features(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(list(zip(*results))[0]).T.tolist()\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(zip(*results))[1]).T.tolist()\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn_crfsuite.CRF(algorithm='lbfgs', # Gradient descent using the limited-memory BFGS method (with L1 and L2 regularization)\n",
    "            c1=0.01,\n",
    "            c2=0.01,\n",
    "            max_iterations=2000,\n",
    "            num_memories=10,\n",
    "            period=10,\n",
    "            epsilon=1e-04,\n",
    "            delta=1e-04,\n",
    "            linesearch='StrongBacktracking',  # 'MoreThuente' 'Backtracking' 'StrongBacktracking'\n",
    "            max_linesearch=20,\n",
    "            all_possible_states=True,\n",
    "            all_possible_transitions=True,\n",
    "            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict_marginals(X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples x time x n classes\n",
    "# pred = np.array([[[ps['land'], ps['cloud']] for ps in p] for p in model.predict_marginals(X)], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the data to predict on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gw.config.update(sensor='rgbn'):\n",
    "    \n",
    "    with gw.open(rasters[0]) as ds:\n",
    "        #print(ds.data.max().compute())\n",
    "        transform = ds.transform\n",
    "    \n",
    "    with gw.open(rasters, time_names=time_names) as ds:\n",
    "        \n",
    "        ds.attrs['transform'] = transform\n",
    "        \n",
    "        dss = ds[:, :, 3000:3512, 3000:3512].chunk((1, 256, 256)).fillna(0)\n",
    "        #dss = gw.subset(ds, left=315416.003, top=-3611632.029, rows=500, cols=500).chunk((1, 1, 64, 64))\n",
    "        #print(dss)\n",
    "        #print(dss.data.max().compute())\n",
    "        test = dss.astype('float64').data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntime, nbands, nrows, ncols = test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the array to CRF-compatible features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [tlayer.transpose(1, 2, 0).reshape(nrows*ncols, nbands) for tlayer in test]\n",
    "features = [array_to_dict(*[features[j][i] for j in range(0, ntime)]) for i in range(0, nrows*ncols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on all time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_marginals(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_labels(model_pred):\n",
    "    \n",
    "    # samples x time x n classes\n",
    "    return np.array([[[ps['n'], ps['l'], ps['w'], ps['c'], ps['s'], ps['h']] for ps in p] for p in model_pred], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples x time x n classes\n",
    "# pred = np.array([[[ps['n'], ps['l'], ps['w'], ps['c'], ps['s'], ps['h']] for ps in p] \n",
    "#                  for p in model.predict_marginals(features)], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred_to_labels(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.transpose(1, 2, 0).reshape(ntime, pred.shape[2], nrows, ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot cloud probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(pred.shape[0], 2, dpi=400, figsize=(2, 12))\n",
    "\n",
    "bidx = 0 # band index\n",
    "cidx = 3 # class index\n",
    "\n",
    "for pi in range(0, pred.shape[0]):\n",
    "\n",
    "    ip = test[pi, bidx, :, :]\n",
    "    ip[ip == 0] = np.nan\n",
    "    \n",
    "    axes[pi][0].imshow(ip, vmin=np.nanpercentile(ip, 10), vmax=np.nanpercentile(ip, 90), cmap='plasma')\n",
    "    #axes[pi][1].imshow(pred[pi, cidx, :, :], vmin=0, vmax=1, cmap='plasma')\n",
    "    axes[pi][1].imshow(pred[pi, :, :, :].argmax(axis=0), vmin=0, vmax=5)\n",
    "    \n",
    "#     axes[pi][0].set_xlabel('Time {:d}'.format(pi+1))\n",
    "    \n",
    "    axes[pi][0].axis('off')\n",
    "    axes[pi][1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.arange(8*3*2).reshape(8,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time 1\n",
    "# a[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time 2\n",
    "# a[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.transpose(1, 2, 0).reshape(3, 2, 2, 4)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
